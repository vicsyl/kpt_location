# maybe put under ds preparation?

# dataset:
dataset:

    # transforms
    reflection: null
    to_grey_scale: True

    # output
    write_metadata: True
    write_data: True
    write_imgs: True
    write_other: True

    # wandb
    enable_wandlog: True
    tags: ["dev"]
    #  "magic_items"
    wandb_run_name_keys: ["no_key_detector", "err_th", "max_files", "reflection", "min_scale_th", "scale_ratio_th"]

    # ds
    augment: lazy # eager, lazy, null
    in_dirs: ["./dataset/raw_data"]
    keys: [""]
    #out_dir: "./dataset/var_sizes"
    #const_patch_size: null
    #out_dir: "./dataset/superpoint_30_files_int_"
    # TODO automate this, too
    out_dir: "./dataset/a_SUPERPOINT_test_"
    max_files: 2
    #ends_with: '.jpg'
    ends_with: '.tonemap.jpg'
    clean_out_dir: True

    # detection
    detector: ADJUSTED_SUPERPOINT # SIFT, SUPERPOINT, ADJUSTED_SUPERPOINT, SIFT_KORNIA, ADJUSTED_SIFT case-insensitive
    scale_ratio_th: null # for superpoint
    min_scale_th: 0.0 # for superpoint
    # scale_ratio_th > 1, scale_ratio \in [1 / scale_ratio_th, scale_ratio_th]
    #scale_ratio_th: 1.1
    #min_scale_th: 15.0
    #min_scale_th: 30.0
    err_th: 5.0
    down_scale: 0.3
    const_patch_size: 33

    # show
    show_kpt_patches: False
    compare_patches: False
    show_inputs: True

    # irrelevant, only defaults
    half_pixel_adjusted: False
    integer_scale: True # try to scale so that the new size is exactly an integer
    dynamic_resizing: False # not for superpoint

    filtering:
        entries: 100
        max_error_distance: 0.071
        sort_error: max # max, null
        heatmap_or_img: heatmap # heatmap, img, both, or null BUT relevant only to SuperPoint
        train_crop: 33
        train_patch_upscale_factor: 3
        train_patch_upscale_method: LANCZOS # LANCZOS, BICUBIC


train:

    # learning
    module: resnet_based #mlp, zero_inference, resnet_based
    accelerator: 'cpu' # ("cpu", "gpu", "tpu", "ipu", "hpu", "mps, "auto")
    gpus: [0]
    batch_size: 16
    max_epochs: 10
    learning_rate: 0.001
    freeze_feature_extractor: False
    scale_error: 1000
    log_every_n_entries: 1024

    # ds
    dataset_splits: 2 # 2 = train, validate, (4 = + test, predict)

    # wandb
    enable_wandlog: True # TODO use for ds as well
    tags: [ "train-ref" ]
    wandb_run_name_keys: [ "magic_out_dir",
                           "train.freeze_feature_extractor",
                           #"train.scale_error",
                           "train.module",
                           "dataset.filtering.entries",
                           "dataset.filtering.heatmap_or_img",
                           "dataset.filtering.train_crop",
                           "dataset.filtering.max_error_distance",
    ]

    # irrelevant, only defaults
    grouped_by_sizes: False
